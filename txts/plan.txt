================================================================================
PLAN: Adapting DINO framework for MRI MHD simulation data
================================================================================
Created: 2026-02-15
Updated: 2026-02-18
Status: IN PROGRESS — standalone flow matching trainer running overfit tests,
        optimization tuning, seeds 0-99 converted (128+256), 100-199 running

================================================================================
QUICK CONTEXT FOR NEW SESSIONS
================================================================================

GOAL: Use the DINO neural operator + diffusion framework (dino/ subdir) to
model our 2D MRI MHD simulation data (from AthenaK, athenak/ subdir).

CURRENT FOCUS: Standalone flow matching super-res (simple_diffusion/), currently
doing overfit tests on single-seed single-channel (velx) to verify training
converges before scaling up. Using only 256 sims: up(down(256)) as input,
native 256 as target. Native 128 sims reserved for later inference testing.

WHAT'S DONE:
  - Simulation parameter search COMPLETE (see searching_for_sim.txt):
      Winner: nu=eta=3e-5, training window orbits 5-50 (snaps 50-500)
      Smallest domain gap between native 128 and downsampled 256
  - 200 seeds at 256x256 + 100 seeds at 128x128, nu=3e-5
    Locations: /mnt/home/mgoldstein/ceph/athenak/mri{256,128}_nu3e-5/seed_*/
  - Seeds 0-99 (both res) fully converted to npy, snaps 0-199 (first 20 orbits)
    256: (200, 256, 256, 8) ~419 MB each, ~42 GB total
    128: (200, 128, 128, 8) ~105 MB each, ~10.5 GB total
  - Paired PyTorch dataloader: simple_diffusion/dataset.py
      Memory-mapped npy (no 52 GB in RAM)
      Per-channel normalization (Welford online stats from training data)
      Channel subsetting (e.g. --channels velx or --channels Bx,jz)
      Returns triplet: up(128), native_256, up(down(256))
      Split by seed (80/10/10), power spectra demo
  - processing/ subdir for data pipeline (convert_to_npy.py, batch_convert.sh)
  - Old pipeline (plotting/) still works for feb13 2048x2048 data
  - simple_diffusion/ standalone flow matching super-res (DINO-independent)
      Flow matching with ODE + SDE sampling (score derived from velocity)
      EMA model weights, LR warmup, bf16 autocast, loss scaling, grad clipping
      Wandb logging: comparison figs, difference figs, power spectra, MSE metrics
      See simple_diffusion/README.md for full details
  - dino/configs/ for diffusion and neural operator configs

WHAT'S NEXT (near-term):
  1. Finish overfit tests (single seed, single channel, ~50k steps)
  2. Scale to multi-seed full training (80 train / 10 val / 10 test seeds)
  3. Scale to all 8 channels
  4. Test inference on native 128 sims (domain gap evaluation)
  5. Seeds 100-199 at 256 still running (more training data later)

WHAT'S NEXT (longer-term):
  - 2-stage pipeline: PINN/PINO neural operator -> diffusion (like DINO)
  - More careful resolution/viscosity selection:
      Fix res (e.g. 256), increase nu until power spectrum converges.
      This ensures the high-res simulation is numerically converged.
  - Evaluate whether 128 and 256 have similar enough physics that a model
    trained on (up(down(256)), 256) is meaningful when applied to up(128).
  - (Optional) Unpaired 128/256 training signal (same seed, diverged physics)

KEY COMMANDS:
  module load python/3.11.11

  # Convert one seed (snaps 50-500):
  python processing/convert_to_npy.py \
    --bindir .../mri256_nu3e-5/seed_0000/bin \
    --outdir .../mri256_nu3e-5/seed_0000/npy \
    --snap-start 50 --snap-end 500

  # Batch all seeds:
  for d in .../mri256_nu3e-5/seed_*/; do
    python processing/convert_to_npy.py \
      --bindir "$d/bin" --outdir "$d/npy" \
      --snap-start 50 --snap-end 500 &
  done; wait

SEE SECTIONS BELOW FOR FULL DETAILS.
SEE ALSO: searching_for_sim.txt for simulation parameter search results.

================================================================================
1. PROJECT CONTEXT
================================================================================

SIMULATION (AthenaK):
- 2D MRI (Magnetorotational Instability) in a shearing box
- Resolution: 2048x2048, 4 GPUs (64 meshblocks of 256x256 across 4 ranks)
- Domain: [-0.5, 0.5] x [-0.5, 0.5], Lx=Ly=1.0
- Physics: isothermal MHD, cs=1.0, viscosity=1e-4, resistivity=1e-4
- Shearing box: qshear=1.5, omega0=1.0, beta=4000
- Time limit: 628.31854 (= 100 orbits, T_orb = 2*pi)
- Output dt = 0.62831853 (= 0.1 orbits) -> 1001 snapshots (00000..01000)
- 3 variable groups:
    mhd_w   -> primitive vars: dens, velx, vely, velz
    mhd_bcc -> cell-centered B: bcc1, bcc2, bcc3
    mhd_jz  -> current density: jz
- Data: /mnt/home/mgoldstein/ceph/athenak/feb13/bin/
- 4 ranks: rank_00000000..rank_00000003

DINO FRAMEWORK:
- Hybrid PINO (Physics-Informed Neural Operator) + Conditional Diffusion
- Originally for 2D INCOMPRESSIBLE MHD with vector potential (u, v, A)
- Two fully separate training paths controlled by config_type:
    "diffusion"  -> train_diffusion() [DiffusionDataset, UNet, MSE loss]
    default      -> train_neural_operator() [MHDDataset, TFNO/FNO, MSE or PINN]

KEY PHYSICS DIFFERENCE:
- DINO: incompressible, constant density, vector potential A_z
- MRI: compressible isothermal, shearing box (Coriolis + tidal + shear)
- This ONLY matters for the PINN loss (deferred)
- For data-driven training (MSE loss, diffusion), physics doesn't matter

================================================================================
2. DATA CONVERSION (COMPLETE)
================================================================================

Script: plotting/convert_to_npy.py
Completed: 2026-02-15, took ~50 minutes

CONVERTED DATA:
  /mnt/home/mgoldstein/ceph/athenak/feb13/npy/all_fields_256x256.npy
    shape: (1001, 256, 256, 8), dtype: float32, size: 2.1 GB
  /mnt/home/mgoldstein/ceph/athenak/feb13/npy/metadata.npz
    field_names, times, x_coords, y_coords, sim_params

ALL 8 FIELDS (in order, as stored in the npy):
  Index  Label     Source       Physical meaning
  -----  --------  -----------  --------------------------------
  0      density   mhd_w/dens   Mass density rho
  1      velx      mhd_w/velx   Radial velocity vx
  2      vely      mhd_w/vely   Azimuthal velocity vy
  3      velz      mhd_w/velz   Vertical velocity vz
  4      Bx        mhd_bcc/bcc1 Radial magnetic field
  5      By        mhd_bcc/bcc2 Azimuthal magnetic field
  6      Bz        mhd_bcc/bcc3 Vertical magnetic field
  7      jz        mhd_jz/jz    Current density (z-component)

FIELD STATISTICS AT EARLY TIME (snapshot 2, t~1.26):
  density: near 1.0 (std ~1.5e-3)    -- isothermal, nearly uniform
  velx:    O(1e-4)                     -- MRI perturbations still growing
  vely:    O(1e-5)                     -- smaller azimuthal perturbation
  velz:    O(1e-5)                     -- non-zero in 2D (vertical component)
  Bx:      O(1e-7)                     -- very weak radial B at early time
  By:      O(1e-2) (sinusoidal)        -- initial net-zero-flux By field
  Bz:      O(1e-7)                     -- very weak vertical B
  jz:      O(1e-1)                     -- current from initial By gradient

CONVERSION OPTIONS (for re-running):
  module load python/3.11.11
  python3 plotting/convert_to_npy.py --resolution 256  # default
  python3 plotting/convert_to_npy.py --resolution 512  # higher res
  python3 plotting/convert_to_npy.py --fields velx,vely,Bx  # subset

================================================================================
3. DATASET CONSTRUCTION (COMPLETE)
================================================================================

Script: plotting/make_dino_dataset.py
Completed: 2026-02-15

CURRENT DATASET (128x128, 3 channels):
  Created with:
    python3 plotting/make_dino_dataset.py \
      --channels velx,vely,Bx \
      --window 50 --stride 10 \
      --downsample-factor 4 \
      --target-resolution 128

  Selected channels (3):
    Ch 0: velx  (radial velocity)
    Ch 1: vely  (azimuthal velocity)
    Ch 2: Bx    (radial magnetic field)

  Why these 3: Matches DINO's default 3-channel setup. velx and vely are the
  primary dynamical variables for MRI. Bx captures the magnetic field that
  drives the instability. By could be added as a 4th channel later.

  Sliding window: 96 samples from 1001 snapshots
    window = 50 timesteps (= 5 orbits each)
    stride = 10 timesteps (= 1 orbit shift)

  Train/val/test split: CONTIGUOUS (not shuffled)
    train: 67 samples  (snapshots 0-669, t = 0 to ~42 orbits)
    val:   14 samples  (snapshots 670-809, t = ~42 to ~51 orbits)
    test:  15 samples  (snapshots 810-1000, t = ~51 to ~63 orbits)

  KNOWN ISSUE -- AMPLITUDE MISMATCH BETWEEN SPLITS:
    The MRI grows exponentially from small perturbations, then saturates
    into turbulence. The contiguous split means:
      - Training set: covers linear growth AND saturated turbulence
      - Val/test: mostly saturated regime with smaller amplitudes
        (val max ~7.8e-3 vs train max ~3.7e-1)
    This is because the simulation starts with tiny perturbations (O(1e-4))
    that grow over ~10-20 orbits to saturated turbulence (O(0.1-0.3)).
    The training set spans the full dynamic range while val/test see only
    the low-amplitude late-time behavior.
    FIX: re-run with shuffled windows, or skip the early growth phase
    (e.g., --snap-start 200), or use a different splitting strategy.
    For now this is fine for smoke-testing the pipeline.

FILES CREATED:
  /mnt/home/mgoldstein/ceph/athenak/feb13/dino/
    neurops_all.npy          (96, 50, 128, 128, 3)    901 MB
    neurops_train.npy        (67, 50, 128, 128, 3)    629 MB
    neurops_val.npy          (14, 50, 128, 128, 3)    132 MB
    neurops_test.npy         (15, 50, 128, 128, 3)    141 MB
    diffusion_train.npy      dict w/ diff_inputs, diff_targets  1.3 GB
      inputs:  (67, 3, 50, 128, 128)  <- 4x downsampled then upsampled
      targets: (67, 3, 50, 128, 128)  <- native 128x128
    diffusion_val.npy        (14, 3, 50, 128, 128)    263 MB
    diffusion_test.npy       (15, 3, 50, 128, 128)    282 MB
    dataset_info.npz         channel_names, window, stride, split info
    stats/
      train_neurops_stats.npz
        mean:    [ 8.0e-07,  3.0e-05, -8.0e-07]
        std:     [ 1.2e-02,  1.5e-02,  6.1e-03]
        min_val: [-3.5e-01, -2.4e-01, -1.6e-01]
        max_val: [ 3.7e-01,  3.1e-01,  1.6e-01]
      train_diffusion_inputs_stats.npz   (same format, for blurred inputs)
      train_diffusion_targets_stats.npz  (same format, for hi-res targets)

  Normalization stats format (compatible with DINO):
    Keys: mean, std, min_val, max_val  -- each shape (C,) = (3,)

RE-GENERATING WITH DIFFERENT CHANNELS:
  The 256x256 npy has all 8 fields, so regeneration is fast (~minutes):
    python3 plotting/make_dino_dataset.py \
      --channels velx,vely,Bx,By \         # 4 channels
      --target-resolution 128 \
      --outdir .../dino_4ch
  Then update config YAML: model_params.channels = 4

  Possible channel selections:
    3ch: velx, vely, Bx               (current, matches DINO default)
    4ch: velx, vely, Bx, By           (add azimuthal B)
    5ch: density, velx, vely, Bx, By  (add compressibility)
    7ch: density, velx, vely, velz, Bx, By, Bz  (everything but jz)
    8ch: all fields                    (maximum information)

================================================================================
4. WHAT NEEDS CODE CHANGES vs. CONFIG-ONLY
================================================================================

CONFIG-ONLY (no DINO source code edits):
  - Diffusion model channel count: model_params.channels in YAML
  - Neural operator channel count: model_params.in_channels, out_channels in YAML
  - Disabling PINN: loss_params.type = "mse" (or just omit physics-informed)
  - Image size: model_params.image_size in YAML
  - Data paths, batch sizes, learning rates, etc.: all YAML
  - The run_training.py entry point routes to the right training function
    based on config_type: "diffusion" vs. default

  KEY: Both training paths (diffusion and neural operator) already exist as
  fully separate, independent codepaths. This is genuinely config-only --
  no flags to add, no code to comment out. The diffusion path has NEVER
  had PINN in it. The neural operator path switches between MSE and PINN
  purely based on loss_params.type in the config YAML.

NO CODE CHANGES NEEDED FOR PHASES 1-3:
  As long as we:
  - Prepare data in the expected .npy format/shape
  - Set channels correctly in config
  - Set image_size correctly (128 or 256, must be divisible by 2^(len(dim_mults)-1))
  - Use compatible dim_mults for the chosen image_size

DEFERRED (MRI PINN):
  - New file: dino/src/physics/mri_pde_solvers.py
  - New file: dino/src/losses/mri_physics_informed.py
  - Register the new loss type so it can be selected via config

================================================================================
5. PHASE PLAN
================================================================================

PHASE 0: Standalone flow matching super-res [IN PROGRESS — overfit testing]
  Purpose: DINO-independent sanity check — minimal code, fast iteration.
  Lives in simple_diffusion/ (not part of DINO framework).
  [x] Write simple_diffusion/model.py — UNet adapted from gf3/models/mg_unet.py
  [x] Write simple_diffusion/trainer.py — flow matching training + ODE/SDE sampling
  [x] Write simple_diffusion/dataset.py — paired dataloader + normalization
  [x] Write simple_diffusion/README.md
  [x] Channel subsetting (--channels velx, default single channel for fast debug)
  [x] Per-channel normalization (Welford online mean/std)
  [x] Base distribution: x_lo_plus_noise (x0 = x_lo + sigma*noise, sigma=0.1)
  [x] SDE sampler: score derived from velocity, no extra training needed
      s(t,x) = -(x - t*v - x_lo) / ((1-t) * sigma^2)
      dx = [v + delta*s] dt + sqrt(2*delta) dW, delta=0.1 default
  [x] EMA model weights (decay=0.9999), samples from both raw + EMA
  [x] Optimization: AdamW, LR warmup (10k steps, skip in overfit), bf16 autocast,
      loss scaling (100x), grad clipping (norm 1.0), t_min/t_max for train+sample
  [x] Wandb viz: comparison figs (ODE+SDE in same plot), difference figs,
      power spectra (truth/ODE/EMA/SDE/low-res), MSE metrics
  [x] Power spectra: low-res computed at native 128 (down(256)), not upsampled,
      to avoid bilinear interpolation artifacts in high-k
  [ ] Verify overfit test converges (loss -> 0, samples match ground truth)
  [ ] Full training run on multi-seed data
  [ ] Test inference on native 128 sims (domain gap)

PHASE 1: Data pipeline [COMPLETE, REORGANIZED 2026-02-17]
  [x] Original: plotting/convert_to_npy.py (for feb13 2048x2048 data)
  [x] New: processing/convert_to_npy.py (for multi-seed workflow)
      - --bindir and --outdir required (no hardcoded defaults)
      - --resolution defaults to native (no downsampling)
      - Single-rank fallback for 1-GPU seed runs
      - Auto-detects native resolution from first snapshot
  [x] processing/make_dino_dataset.py copied from plotting/ (not yet updated)

PHASE 1.5: Simulation parameter search [COMPLETE 2026-02-16]
  See searching_for_sim.txt for full details.
  [x] Swept nu=eta: 1e-3, 1e-4, 5e-5, 3e-5, 1e-5 at 128 and 256
  [x] Winner: nu=eta=3e-5 (smallest domain gap, slowest decay)
  [x] Training window: orbits 5-50 (snaps 50-500)
  [x] Input decks: decks/mri2d.athinput.{128,256}
  [x] Launch scripts: toy_sweep.sbatch + toy.sh

PHASE 1.75: Production seed runs [SEEDS 0-99 COMPLETE + CONVERTED, 100-199 RUNNING]
  [x] Launched ~200 seeds at 256x256, nu=eta=3e-5, 1 GPU each
      Location: /mnt/home/mgoldstein/ceph/athenak/mri256_nu3e-5/seed_*/
      Runtime: ~45 min per seed (50 orbits = 314 sim seconds)
  [x] Seeds 0-99 finished
  [x] Converted seeds 0-99 snaps 0-199 to npy (2026-02-17)
      Each: (200, 256, 256, 8) float32, ~0.42 GB, ~42 GB total
      Location: .../seed_NNNN/npy/all_fields_256x256.npy + metadata.npz
      Ran in parallel batches of 10, ~35s per batch
  [ ] Seeds 100-199 still running (do not touch)
  [x] 100 seeds at 128x128, nu=eta=3e-5, 1 GPU each — COMPLETE
      Location: /mnt/home/mgoldstein/ceph/athenak/mri128_nu3e-5/seed_*/
  [x] Converted 128 seeds 0-99 snaps 0-199 to npy (2026-02-17)
      Each: (200, 128, 128, 8) float32, ~0.10 GB, ~10.5 GB total
      Location: .../seed_NNNN/npy/all_fields_128x128.npy + metadata.npz
  [x] Conversion script: processing/batch_convert.sh (batches of 10 in parallel)

PHASE 2: Dataset creation [COMPLETE — paired dataloader in simple_diffusion/]
  [x] plotting/make_dino_dataset.py works for single-run data (feb13)
  [x] simple_diffusion/dataset.py — paired PyTorch DataLoader for super-res
      - MRIPairedDataset: memory-mapped npy, returns (native_128, native_256)
      - prepare_batch(): GPU-side up/down, returns triplet:
          up_128:      native 128 upsampled to 256  (inference input)
          x_256:       native 256                    (target)
          up_down_256: 256 avg-pooled->128->256      (training input)
      - make_loaders(): convenience fn, default 80/10/10 seed split
      - Demo: power spectra, domain gap analysis, comparison figures
      - Split by SEED to avoid leakage
  [ ] (Optional) Update processing/make_dino_dataset.py for DINO framework

PHASE 3: Diffusion super-resolution (first ML test) [READY]
  [x] Write dino/configs/config_diffusion_mri.yaml
  [ ] Test training run with simple_diffusion/ on multi-seed data

PHASE 4: Neural operator forecasting (MSE only)
  [x] Write dino/configs/config_neurops_mri.yaml
  [ ] Train and evaluate

PHASE 5 (later): MRI PINN loss

PHASE 6 (later): Full PINO + Diffusion pipeline

================================================================================
6. FILES CREATED
================================================================================

PROCESSING SCRIPTS (new, multi-seed workflow):
  processing/convert_to_npy.py            # bin -> npy, per-seed, snap range args
  processing/batch_convert.sh             # batch convert seeds in parallel (10 at a time)
  processing/make_dino_dataset.py         # npy -> DINO format (copied, needs update)

PLOTTING SCRIPTS (original, feb13 data):
  plotting/convert_to_npy.py              # bin -> npy (hardcoded feb13 defaults)
  plotting/make_dino_dataset.py           # npy -> DINO dataset formatter
  plotting/make_video.py                  # mp4 from raw bins
  plotting/make_video_from_npy.py         # mp4 from npy (faster)
  plotting/domain_gap_analysis.py         # domain gap comparison
  plotting/analyze_sweep.py              # turbulence sustainability

ML CODE (simple_diffusion/):
  simple_diffusion/model.py               # UNet (from gf3), ~43M (8ch) / ~40M (1ch)
  simple_diffusion/trainer.py             # flow matching: train, ODE+SDE sample, EMA, viz
  simple_diffusion/dataset.py             # paired DataLoader, normalize/denormalize, stats
  simple_diffusion/README.md              # full documentation of method + config

CONFIGS:
  dino/configs/config_diffusion_mri.yaml  # diffusion super-res config
  dino/configs/config_neurops_mri.yaml    # neural operator MSE config

SIMULATION LAUNCH:
  decks/mri2d.athinput.{128,256}          # input decks for 128/256 runs
  toy_sweep.sbatch                        # parameterized sbatch (RES, NU env vars)
  toy.sh                                  # launches nu/eta sweep

DATA ON CEPH (feb13, 2048x2048, old):
  /mnt/home/mgoldstein/ceph/athenak/feb13/npy/
    all_fields_256x256.npy                # (1001, 256, 256, 8) 2.1 GB
    metadata.npz                          # field names, times, coords
  /mnt/home/mgoldstein/ceph/athenak/feb13/dino/
    neurops_all.npy                       # (96, 50, 128, 128, 3) 901 MB
    neurops_{train,val,test}.npy          # per-split
    diffusion_{train,val,test}.npy        # dict: diff_inputs, diff_targets
    dataset_info.npz                      # split info, params
    stats/                                # normalization stats

DATA ON CEPH (production seeds, nu=3e-5):
  /mnt/home/mgoldstein/ceph/athenak/mri256_nu3e-5/seed_*/
    bin/                                  # raw AthenaK output
    npy/all_fields_256x256.npy            # (200, 256, 256, 8) snaps 0-199
    npy/metadata.npz                      # field names, times, coords
    Seeds 0-99: converted. Seeds 100-199: bins only (still running)
  /mnt/home/mgoldstein/ceph/athenak/mri128_nu3e-5/seed_*/
    bin/                                  # raw AthenaK output
    npy/all_fields_128x128.npy            # (200, 128, 128, 8) snaps 0-199
    npy/metadata.npz
    Seeds 0-99: all converted

DEFERRED:
  dino/src/physics/mri_pde_solvers.py     # MRI PINN (Phase 5)
  dino/src/losses/mri_physics_informed.py # MRI loss (Phase 5)

================================================================================
7. HOW TO RUN
================================================================================

ENVIRONMENT:
  module load python/3.11.11   # needed for h5py, scipy, numpy, torch

--- NEW WORKFLOW (multi-seed, nu=3e-5) ---

STEP 1 - Seeds are running (~45 min each for 50 orbits):
  Location: /mnt/home/mgoldstein/ceph/athenak/mri256_nu3e-5/seed_*/

STEP 2 - Convert seed bins to npy (snaps 50-500):
  # Single seed:
  python3 processing/convert_to_npy.py \
    --bindir /mnt/home/mgoldstein/ceph/athenak/mri256_nu3e-5/seed_0000/bin \
    --outdir /mnt/home/mgoldstein/ceph/athenak/mri256_nu3e-5/seed_0000/npy \
    --snap-start 50 --snap-end 500

  # Batch all seeds in parallel:
  for d in /mnt/home/mgoldstein/ceph/athenak/mri256_nu3e-5/seed_*/; do
    python3 processing/convert_to_npy.py \
      --bindir "$d/bin" --outdir "$d/npy" \
      --snap-start 50 --snap-end 500 &
  done; wait

STEP 3 - Train standalone flow matching super-res:
  source load.sh  # or: module load python/3.11.11

  # Overfit test (single seed, single channel, default):
  python3 simple_diffusion/trainer.py

  # Custom channel / settings:
  python3 simple_diffusion/trainer.py --channels Bx --sde-delta 0.2

  # Full training (all channels, multi-seed):
  python3 simple_diffusion/trainer.py \
    --channels density,velx,vely,velz,Bx,By,Bz,jz --no-overfit

STEP 4 - (Later) DINO framework training

--- OLD WORKFLOW (feb13 2048x2048 single run) ---

STEP 1 - Convert simulation data (already done):
  python3 plotting/convert_to_npy.py --resolution 256

STEP 2 - Create DINO datasets (already done):
  python3 plotting/make_dino_dataset.py \
    --channels velx,vely,Bx \
    --window 50 --stride 10 \
    --target-resolution 128 \
    --downsample-factor 4

STEP 3a - Standalone flow matching (simple_diffusion/, DINO-independent):
  source load.sh
  python3 simple_diffusion/trainer.py

STEP 3b - Train diffusion model (DINO framework):
  cd dino
  python3 run_training.py --config configs/config_diffusion_mri.yaml

STEP 4 - Train neural operator:
  cd dino
  python3 run_training.py --config configs/config_neurops_mri.yaml

================================================================================
END OF PLAN
================================================================================
